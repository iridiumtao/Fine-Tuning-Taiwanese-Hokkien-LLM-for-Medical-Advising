{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.996814159292035,
  "eval_steps": 500,
  "global_step": 2118,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01415929203539823,
      "grad_norm": 0.6074230074882507,
      "learning_rate": 4.9787535410764873e-05,
      "loss": 1.9904,
      "step": 10
    },
    {
      "epoch": 0.02831858407079646,
      "grad_norm": 0.9433989524841309,
      "learning_rate": 4.9551463644948064e-05,
      "loss": 1.8489,
      "step": 20
    },
    {
      "epoch": 0.04247787610619469,
      "grad_norm": 0.9606435298919678,
      "learning_rate": 4.9315391879131255e-05,
      "loss": 1.5599,
      "step": 30
    },
    {
      "epoch": 0.05663716814159292,
      "grad_norm": 0.5487502217292786,
      "learning_rate": 4.9079320113314446e-05,
      "loss": 1.354,
      "step": 40
    },
    {
      "epoch": 0.07079646017699115,
      "grad_norm": 0.5273997783660889,
      "learning_rate": 4.884324834749764e-05,
      "loss": 1.2895,
      "step": 50
    },
    {
      "epoch": 0.08495575221238938,
      "grad_norm": 0.407720148563385,
      "learning_rate": 4.8607176581680834e-05,
      "loss": 1.2534,
      "step": 60
    },
    {
      "epoch": 0.09911504424778761,
      "grad_norm": 0.5027709603309631,
      "learning_rate": 4.8371104815864025e-05,
      "loss": 1.2338,
      "step": 70
    },
    {
      "epoch": 0.11327433628318584,
      "grad_norm": 0.4705567955970764,
      "learning_rate": 4.8135033050047215e-05,
      "loss": 1.2218,
      "step": 80
    },
    {
      "epoch": 0.12743362831858407,
      "grad_norm": 0.4506894648075104,
      "learning_rate": 4.7898961284230406e-05,
      "loss": 1.2476,
      "step": 90
    },
    {
      "epoch": 0.1415929203539823,
      "grad_norm": 0.47130367159843445,
      "learning_rate": 4.7662889518413604e-05,
      "loss": 1.2997,
      "step": 100
    },
    {
      "epoch": 0.15575221238938053,
      "grad_norm": 0.4246595799922943,
      "learning_rate": 4.7426817752596795e-05,
      "loss": 1.2317,
      "step": 110
    },
    {
      "epoch": 0.16991150442477876,
      "grad_norm": 0.4373388886451721,
      "learning_rate": 4.7190745986779985e-05,
      "loss": 1.2303,
      "step": 120
    },
    {
      "epoch": 0.184070796460177,
      "grad_norm": 0.44644346833229065,
      "learning_rate": 4.6954674220963176e-05,
      "loss": 1.2023,
      "step": 130
    },
    {
      "epoch": 0.19823008849557522,
      "grad_norm": 0.4124859571456909,
      "learning_rate": 4.671860245514637e-05,
      "loss": 1.2415,
      "step": 140
    },
    {
      "epoch": 0.21238938053097345,
      "grad_norm": 0.5077070593833923,
      "learning_rate": 4.648253068932956e-05,
      "loss": 1.2018,
      "step": 150
    },
    {
      "epoch": 0.22654867256637168,
      "grad_norm": 0.39725497364997864,
      "learning_rate": 4.624645892351275e-05,
      "loss": 1.1747,
      "step": 160
    },
    {
      "epoch": 0.2407079646017699,
      "grad_norm": 0.5178326368331909,
      "learning_rate": 4.6010387157695946e-05,
      "loss": 1.2019,
      "step": 170
    },
    {
      "epoch": 0.25486725663716814,
      "grad_norm": 0.5021690726280212,
      "learning_rate": 4.5774315391879137e-05,
      "loss": 1.224,
      "step": 180
    },
    {
      "epoch": 0.26902654867256637,
      "grad_norm": 0.4753296971321106,
      "learning_rate": 4.553824362606233e-05,
      "loss": 1.1965,
      "step": 190
    },
    {
      "epoch": 0.2831858407079646,
      "grad_norm": 0.46761491894721985,
      "learning_rate": 4.530217186024552e-05,
      "loss": 1.1776,
      "step": 200
    },
    {
      "epoch": 0.2973451327433628,
      "grad_norm": 0.41964372992515564,
      "learning_rate": 4.506610009442871e-05,
      "loss": 1.229,
      "step": 210
    },
    {
      "epoch": 0.31150442477876106,
      "grad_norm": 0.47693344950675964,
      "learning_rate": 4.48300283286119e-05,
      "loss": 1.1902,
      "step": 220
    },
    {
      "epoch": 0.3256637168141593,
      "grad_norm": 0.4330342710018158,
      "learning_rate": 4.459395656279509e-05,
      "loss": 1.2123,
      "step": 230
    },
    {
      "epoch": 0.3398230088495575,
      "grad_norm": 0.46605512499809265,
      "learning_rate": 4.435788479697828e-05,
      "loss": 1.2135,
      "step": 240
    },
    {
      "epoch": 0.35398230088495575,
      "grad_norm": 0.521269679069519,
      "learning_rate": 4.412181303116147e-05,
      "loss": 1.1978,
      "step": 250
    },
    {
      "epoch": 0.368141592920354,
      "grad_norm": 0.5069058537483215,
      "learning_rate": 4.388574126534466e-05,
      "loss": 1.2004,
      "step": 260
    },
    {
      "epoch": 0.3823008849557522,
      "grad_norm": 0.4877839982509613,
      "learning_rate": 4.3649669499527853e-05,
      "loss": 1.1666,
      "step": 270
    },
    {
      "epoch": 0.39646017699115044,
      "grad_norm": 0.5884671211242676,
      "learning_rate": 4.341359773371105e-05,
      "loss": 1.167,
      "step": 280
    },
    {
      "epoch": 0.41061946902654867,
      "grad_norm": 0.5354362726211548,
      "learning_rate": 4.317752596789424e-05,
      "loss": 1.2062,
      "step": 290
    },
    {
      "epoch": 0.4247787610619469,
      "grad_norm": 0.5312383770942688,
      "learning_rate": 4.294145420207743e-05,
      "loss": 1.1337,
      "step": 300
    },
    {
      "epoch": 0.4389380530973451,
      "grad_norm": 0.46409180760383606,
      "learning_rate": 4.270538243626063e-05,
      "loss": 1.2202,
      "step": 310
    },
    {
      "epoch": 0.45309734513274336,
      "grad_norm": 0.5317000150680542,
      "learning_rate": 4.246931067044382e-05,
      "loss": 1.2394,
      "step": 320
    },
    {
      "epoch": 0.4672566371681416,
      "grad_norm": 0.5490352511405945,
      "learning_rate": 4.223323890462701e-05,
      "loss": 1.2414,
      "step": 330
    },
    {
      "epoch": 0.4814159292035398,
      "grad_norm": 0.5651182532310486,
      "learning_rate": 4.19971671388102e-05,
      "loss": 1.1917,
      "step": 340
    },
    {
      "epoch": 0.49557522123893805,
      "grad_norm": 0.5097593665122986,
      "learning_rate": 4.176109537299339e-05,
      "loss": 1.2507,
      "step": 350
    },
    {
      "epoch": 0.5097345132743363,
      "grad_norm": 0.48467448353767395,
      "learning_rate": 4.1525023607176584e-05,
      "loss": 1.2334,
      "step": 360
    },
    {
      "epoch": 0.5238938053097345,
      "grad_norm": 0.5029170513153076,
      "learning_rate": 4.1288951841359775e-05,
      "loss": 1.1914,
      "step": 370
    },
    {
      "epoch": 0.5380530973451327,
      "grad_norm": 0.6601401567459106,
      "learning_rate": 4.1052880075542965e-05,
      "loss": 1.1782,
      "step": 380
    },
    {
      "epoch": 0.552212389380531,
      "grad_norm": 0.6263619065284729,
      "learning_rate": 4.0816808309726156e-05,
      "loss": 1.1942,
      "step": 390
    },
    {
      "epoch": 0.5663716814159292,
      "grad_norm": 0.5382691025733948,
      "learning_rate": 4.058073654390935e-05,
      "loss": 1.1747,
      "step": 400
    },
    {
      "epoch": 0.5805309734513274,
      "grad_norm": 0.506964385509491,
      "learning_rate": 4.0344664778092544e-05,
      "loss": 1.1283,
      "step": 410
    },
    {
      "epoch": 0.5946902654867257,
      "grad_norm": 0.5765085220336914,
      "learning_rate": 4.0108593012275735e-05,
      "loss": 1.1489,
      "step": 420
    },
    {
      "epoch": 0.6088495575221239,
      "grad_norm": 0.5110203623771667,
      "learning_rate": 3.9872521246458926e-05,
      "loss": 1.1688,
      "step": 430
    },
    {
      "epoch": 0.6230088495575221,
      "grad_norm": 0.5641288757324219,
      "learning_rate": 3.963644948064212e-05,
      "loss": 1.1964,
      "step": 440
    },
    {
      "epoch": 0.6371681415929203,
      "grad_norm": 0.6234747171401978,
      "learning_rate": 3.940037771482531e-05,
      "loss": 1.2087,
      "step": 450
    },
    {
      "epoch": 0.6513274336283186,
      "grad_norm": 0.5322465896606445,
      "learning_rate": 3.91643059490085e-05,
      "loss": 1.1983,
      "step": 460
    },
    {
      "epoch": 0.6654867256637168,
      "grad_norm": 0.596155047416687,
      "learning_rate": 3.892823418319169e-05,
      "loss": 1.26,
      "step": 470
    },
    {
      "epoch": 0.679646017699115,
      "grad_norm": 0.5322378873825073,
      "learning_rate": 3.8692162417374886e-05,
      "loss": 1.179,
      "step": 480
    },
    {
      "epoch": 0.6938053097345133,
      "grad_norm": 0.5768598318099976,
      "learning_rate": 3.845609065155808e-05,
      "loss": 1.1371,
      "step": 490
    },
    {
      "epoch": 0.7079646017699115,
      "grad_norm": 0.6027250289916992,
      "learning_rate": 3.822001888574127e-05,
      "loss": 1.2154,
      "step": 500
    },
    {
      "epoch": 0.7221238938053097,
      "grad_norm": 0.5880152583122253,
      "learning_rate": 3.798394711992446e-05,
      "loss": 1.1902,
      "step": 510
    },
    {
      "epoch": 0.736283185840708,
      "grad_norm": 0.5748769640922546,
      "learning_rate": 3.774787535410765e-05,
      "loss": 1.1628,
      "step": 520
    },
    {
      "epoch": 0.7504424778761062,
      "grad_norm": 0.501137375831604,
      "learning_rate": 3.751180358829085e-05,
      "loss": 1.1874,
      "step": 530
    },
    {
      "epoch": 0.7646017699115044,
      "grad_norm": 0.5487127304077148,
      "learning_rate": 3.727573182247404e-05,
      "loss": 1.1806,
      "step": 540
    },
    {
      "epoch": 0.7787610619469026,
      "grad_norm": 0.5502758622169495,
      "learning_rate": 3.703966005665723e-05,
      "loss": 1.1949,
      "step": 550
    },
    {
      "epoch": 0.7929203539823009,
      "grad_norm": 0.6300752758979797,
      "learning_rate": 3.680358829084042e-05,
      "loss": 1.1515,
      "step": 560
    },
    {
      "epoch": 0.8070796460176991,
      "grad_norm": 0.6122263073921204,
      "learning_rate": 3.656751652502361e-05,
      "loss": 1.213,
      "step": 570
    },
    {
      "epoch": 0.8212389380530973,
      "grad_norm": 0.5451769232749939,
      "learning_rate": 3.63314447592068e-05,
      "loss": 1.2041,
      "step": 580
    },
    {
      "epoch": 0.8353982300884956,
      "grad_norm": 0.6244900822639465,
      "learning_rate": 3.609537299338999e-05,
      "loss": 1.1979,
      "step": 590
    },
    {
      "epoch": 0.8495575221238938,
      "grad_norm": 0.5876796245574951,
      "learning_rate": 3.585930122757318e-05,
      "loss": 1.1904,
      "step": 600
    },
    {
      "epoch": 0.863716814159292,
      "grad_norm": 0.5295788645744324,
      "learning_rate": 3.562322946175637e-05,
      "loss": 1.1826,
      "step": 610
    },
    {
      "epoch": 0.8778761061946903,
      "grad_norm": 0.5863450765609741,
      "learning_rate": 3.5387157695939564e-05,
      "loss": 1.1748,
      "step": 620
    },
    {
      "epoch": 0.8920353982300885,
      "grad_norm": 0.6826558709144592,
      "learning_rate": 3.5151085930122755e-05,
      "loss": 1.197,
      "step": 630
    },
    {
      "epoch": 0.9061946902654867,
      "grad_norm": 0.539742112159729,
      "learning_rate": 3.4915014164305945e-05,
      "loss": 1.1591,
      "step": 640
    },
    {
      "epoch": 0.9203539823008849,
      "grad_norm": 0.5767478942871094,
      "learning_rate": 3.467894239848914e-05,
      "loss": 1.1805,
      "step": 650
    },
    {
      "epoch": 0.9345132743362832,
      "grad_norm": 0.5880154967308044,
      "learning_rate": 3.4442870632672334e-05,
      "loss": 1.171,
      "step": 660
    },
    {
      "epoch": 0.9486725663716814,
      "grad_norm": 0.5599102973937988,
      "learning_rate": 3.4206798866855524e-05,
      "loss": 1.0795,
      "step": 670
    },
    {
      "epoch": 0.9628318584070796,
      "grad_norm": 0.5402117371559143,
      "learning_rate": 3.3970727101038715e-05,
      "loss": 1.12,
      "step": 680
    },
    {
      "epoch": 0.9769911504424779,
      "grad_norm": 0.6729663610458374,
      "learning_rate": 3.373465533522191e-05,
      "loss": 1.1729,
      "step": 690
    },
    {
      "epoch": 0.9911504424778761,
      "grad_norm": 0.6609266996383667,
      "learning_rate": 3.3498583569405103e-05,
      "loss": 1.1561,
      "step": 700
    },
    {
      "epoch": 1.0042477876106195,
      "grad_norm": 0.5668928027153015,
      "learning_rate": 3.3262511803588294e-05,
      "loss": 1.1768,
      "step": 710
    },
    {
      "epoch": 1.0184070796460176,
      "grad_norm": 0.596657931804657,
      "learning_rate": 3.3026440037771485e-05,
      "loss": 1.1932,
      "step": 720
    },
    {
      "epoch": 1.032566371681416,
      "grad_norm": 0.569480836391449,
      "learning_rate": 3.2790368271954676e-05,
      "loss": 1.1368,
      "step": 730
    },
    {
      "epoch": 1.046725663716814,
      "grad_norm": 0.6038028597831726,
      "learning_rate": 3.2554296506137866e-05,
      "loss": 1.1701,
      "step": 740
    },
    {
      "epoch": 1.0608849557522124,
      "grad_norm": 0.5429558157920837,
      "learning_rate": 3.231822474032106e-05,
      "loss": 1.182,
      "step": 750
    },
    {
      "epoch": 1.0750442477876105,
      "grad_norm": 0.6025366187095642,
      "learning_rate": 3.208215297450425e-05,
      "loss": 1.161,
      "step": 760
    },
    {
      "epoch": 1.0892035398230089,
      "grad_norm": 0.6434605121612549,
      "learning_rate": 3.1846081208687446e-05,
      "loss": 1.2296,
      "step": 770
    },
    {
      "epoch": 1.103362831858407,
      "grad_norm": 0.6150287389755249,
      "learning_rate": 3.1610009442870636e-05,
      "loss": 1.1502,
      "step": 780
    },
    {
      "epoch": 1.1175221238938053,
      "grad_norm": 0.6670460104942322,
      "learning_rate": 3.137393767705383e-05,
      "loss": 1.144,
      "step": 790
    },
    {
      "epoch": 1.1316814159292035,
      "grad_norm": 0.6282586455345154,
      "learning_rate": 3.113786591123702e-05,
      "loss": 1.099,
      "step": 800
    },
    {
      "epoch": 1.1458407079646018,
      "grad_norm": 0.5764253735542297,
      "learning_rate": 3.090179414542021e-05,
      "loss": 1.1321,
      "step": 810
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.5913510322570801,
      "learning_rate": 3.06657223796034e-05,
      "loss": 1.1599,
      "step": 820
    },
    {
      "epoch": 1.1741592920353983,
      "grad_norm": 0.5901618599891663,
      "learning_rate": 3.0429650613786593e-05,
      "loss": 1.188,
      "step": 830
    },
    {
      "epoch": 1.1883185840707964,
      "grad_norm": 0.6171350479125977,
      "learning_rate": 3.0193578847969784e-05,
      "loss": 1.1282,
      "step": 840
    },
    {
      "epoch": 1.2024778761061947,
      "grad_norm": 0.6954542994499207,
      "learning_rate": 2.9957507082152975e-05,
      "loss": 1.1389,
      "step": 850
    },
    {
      "epoch": 1.2166371681415928,
      "grad_norm": 0.7069576382637024,
      "learning_rate": 2.9721435316336166e-05,
      "loss": 1.1344,
      "step": 860
    },
    {
      "epoch": 1.2307964601769912,
      "grad_norm": 0.6729207038879395,
      "learning_rate": 2.9485363550519356e-05,
      "loss": 1.1646,
      "step": 870
    },
    {
      "epoch": 1.2449557522123893,
      "grad_norm": 0.7099921107292175,
      "learning_rate": 2.9249291784702547e-05,
      "loss": 1.1779,
      "step": 880
    },
    {
      "epoch": 1.2591150442477876,
      "grad_norm": 0.6988247632980347,
      "learning_rate": 2.9013220018885745e-05,
      "loss": 1.0993,
      "step": 890
    },
    {
      "epoch": 1.2732743362831858,
      "grad_norm": 0.6211530566215515,
      "learning_rate": 2.8777148253068936e-05,
      "loss": 1.1542,
      "step": 900
    },
    {
      "epoch": 1.287433628318584,
      "grad_norm": 0.665182888507843,
      "learning_rate": 2.8541076487252126e-05,
      "loss": 1.1677,
      "step": 910
    },
    {
      "epoch": 1.3015929203539822,
      "grad_norm": 0.6815356612205505,
      "learning_rate": 2.8305004721435317e-05,
      "loss": 1.1612,
      "step": 920
    },
    {
      "epoch": 1.3157522123893806,
      "grad_norm": 0.6125563383102417,
      "learning_rate": 2.806893295561851e-05,
      "loss": 1.1638,
      "step": 930
    },
    {
      "epoch": 1.3299115044247787,
      "grad_norm": 0.6609587073326111,
      "learning_rate": 2.7832861189801702e-05,
      "loss": 1.1535,
      "step": 940
    },
    {
      "epoch": 1.344070796460177,
      "grad_norm": 0.6733593344688416,
      "learning_rate": 2.7596789423984893e-05,
      "loss": 1.2012,
      "step": 950
    },
    {
      "epoch": 1.3582300884955751,
      "grad_norm": 0.713232159614563,
      "learning_rate": 2.7360717658168083e-05,
      "loss": 1.1433,
      "step": 960
    },
    {
      "epoch": 1.3723893805309735,
      "grad_norm": 0.7324450612068176,
      "learning_rate": 2.7124645892351274e-05,
      "loss": 1.1692,
      "step": 970
    },
    {
      "epoch": 1.3865486725663716,
      "grad_norm": 0.71825110912323,
      "learning_rate": 2.6888574126534465e-05,
      "loss": 1.1666,
      "step": 980
    },
    {
      "epoch": 1.40070796460177,
      "grad_norm": 0.6834214925765991,
      "learning_rate": 2.6652502360717656e-05,
      "loss": 1.2133,
      "step": 990
    },
    {
      "epoch": 1.414867256637168,
      "grad_norm": 0.7047013640403748,
      "learning_rate": 2.641643059490085e-05,
      "loss": 1.139,
      "step": 1000
    },
    {
      "epoch": 1.4290265486725664,
      "grad_norm": 0.6853421330451965,
      "learning_rate": 2.6180358829084044e-05,
      "loss": 1.1388,
      "step": 1010
    },
    {
      "epoch": 1.4431858407079647,
      "grad_norm": 0.7511932253837585,
      "learning_rate": 2.5944287063267235e-05,
      "loss": 1.1436,
      "step": 1020
    },
    {
      "epoch": 1.4573451327433629,
      "grad_norm": 0.6999959945678711,
      "learning_rate": 2.570821529745043e-05,
      "loss": 1.1547,
      "step": 1030
    },
    {
      "epoch": 1.471504424778761,
      "grad_norm": 0.7413654327392578,
      "learning_rate": 2.547214353163362e-05,
      "loss": 1.179,
      "step": 1040
    },
    {
      "epoch": 1.4856637168141593,
      "grad_norm": 0.6783543229103088,
      "learning_rate": 2.523607176581681e-05,
      "loss": 1.1553,
      "step": 1050
    },
    {
      "epoch": 1.4998230088495577,
      "grad_norm": 0.7313976287841797,
      "learning_rate": 2.5e-05,
      "loss": 1.2079,
      "step": 1060
    },
    {
      "epoch": 1.5139823008849558,
      "grad_norm": 0.6954549551010132,
      "learning_rate": 2.4763928234183192e-05,
      "loss": 1.1845,
      "step": 1070
    },
    {
      "epoch": 1.528141592920354,
      "grad_norm": 0.7039560675621033,
      "learning_rate": 2.4527856468366383e-05,
      "loss": 1.1475,
      "step": 1080
    },
    {
      "epoch": 1.5423008849557522,
      "grad_norm": 0.8317824602127075,
      "learning_rate": 2.4291784702549577e-05,
      "loss": 1.179,
      "step": 1090
    },
    {
      "epoch": 1.5564601769911506,
      "grad_norm": 0.7728369235992432,
      "learning_rate": 2.4055712936732768e-05,
      "loss": 1.1404,
      "step": 1100
    },
    {
      "epoch": 1.5706194690265487,
      "grad_norm": 0.6496664881706238,
      "learning_rate": 2.381964117091596e-05,
      "loss": 1.1926,
      "step": 1110
    },
    {
      "epoch": 1.5847787610619468,
      "grad_norm": 0.7068901062011719,
      "learning_rate": 2.3583569405099153e-05,
      "loss": 1.1328,
      "step": 1120
    },
    {
      "epoch": 1.5989380530973452,
      "grad_norm": 0.6886337399482727,
      "learning_rate": 2.3347497639282343e-05,
      "loss": 1.1019,
      "step": 1130
    },
    {
      "epoch": 1.6130973451327435,
      "grad_norm": 0.7240273952484131,
      "learning_rate": 2.3111425873465534e-05,
      "loss": 1.1673,
      "step": 1140
    },
    {
      "epoch": 1.6272566371681416,
      "grad_norm": 0.6897395253181458,
      "learning_rate": 2.2875354107648728e-05,
      "loss": 1.2143,
      "step": 1150
    },
    {
      "epoch": 1.6414159292035397,
      "grad_norm": 0.6650887727737427,
      "learning_rate": 2.263928234183192e-05,
      "loss": 1.0915,
      "step": 1160
    },
    {
      "epoch": 1.655575221238938,
      "grad_norm": 0.7097872495651245,
      "learning_rate": 2.240321057601511e-05,
      "loss": 1.1589,
      "step": 1170
    },
    {
      "epoch": 1.6697345132743364,
      "grad_norm": 0.7865168452262878,
      "learning_rate": 2.21671388101983e-05,
      "loss": 1.1976,
      "step": 1180
    },
    {
      "epoch": 1.6838938053097345,
      "grad_norm": 0.7466804385185242,
      "learning_rate": 2.193106704438149e-05,
      "loss": 1.1637,
      "step": 1190
    },
    {
      "epoch": 1.6980530973451327,
      "grad_norm": 0.726989209651947,
      "learning_rate": 2.1694995278564685e-05,
      "loss": 1.1572,
      "step": 1200
    },
    {
      "epoch": 1.712212389380531,
      "grad_norm": 0.6982690095901489,
      "learning_rate": 2.1458923512747876e-05,
      "loss": 1.1205,
      "step": 1210
    },
    {
      "epoch": 1.7263716814159293,
      "grad_norm": 0.7609624266624451,
      "learning_rate": 2.122285174693107e-05,
      "loss": 1.1454,
      "step": 1220
    },
    {
      "epoch": 1.7405309734513275,
      "grad_norm": 0.7214741110801697,
      "learning_rate": 2.098677998111426e-05,
      "loss": 1.1691,
      "step": 1230
    },
    {
      "epoch": 1.7546902654867256,
      "grad_norm": 0.7365829348564148,
      "learning_rate": 2.0750708215297452e-05,
      "loss": 1.1541,
      "step": 1240
    },
    {
      "epoch": 1.768849557522124,
      "grad_norm": 0.7134445905685425,
      "learning_rate": 2.0514636449480643e-05,
      "loss": 1.1102,
      "step": 1250
    },
    {
      "epoch": 1.7830088495575223,
      "grad_norm": 0.7848861813545227,
      "learning_rate": 2.0278564683663833e-05,
      "loss": 1.1371,
      "step": 1260
    },
    {
      "epoch": 1.7971681415929204,
      "grad_norm": 0.684727668762207,
      "learning_rate": 2.0042492917847027e-05,
      "loss": 1.1841,
      "step": 1270
    },
    {
      "epoch": 1.8113274336283185,
      "grad_norm": 0.6681351661682129,
      "learning_rate": 1.9806421152030218e-05,
      "loss": 1.111,
      "step": 1280
    },
    {
      "epoch": 1.8254867256637168,
      "grad_norm": 0.7325250506401062,
      "learning_rate": 1.957034938621341e-05,
      "loss": 1.2406,
      "step": 1290
    },
    {
      "epoch": 1.8396460176991152,
      "grad_norm": 0.7404804825782776,
      "learning_rate": 1.93342776203966e-05,
      "loss": 1.1603,
      "step": 1300
    },
    {
      "epoch": 1.8538053097345133,
      "grad_norm": 0.7472787499427795,
      "learning_rate": 1.9098205854579794e-05,
      "loss": 1.1552,
      "step": 1310
    },
    {
      "epoch": 1.8679646017699114,
      "grad_norm": 0.7470943927764893,
      "learning_rate": 1.8862134088762985e-05,
      "loss": 1.1777,
      "step": 1320
    },
    {
      "epoch": 1.8821238938053098,
      "grad_norm": 0.6708048582077026,
      "learning_rate": 1.862606232294618e-05,
      "loss": 1.166,
      "step": 1330
    },
    {
      "epoch": 1.896283185840708,
      "grad_norm": 0.8166793584823608,
      "learning_rate": 1.838999055712937e-05,
      "loss": 1.1371,
      "step": 1340
    },
    {
      "epoch": 1.9104424778761062,
      "grad_norm": 0.7190630435943604,
      "learning_rate": 1.815391879131256e-05,
      "loss": 1.1231,
      "step": 1350
    },
    {
      "epoch": 1.9246017699115043,
      "grad_norm": 0.7053236961364746,
      "learning_rate": 1.791784702549575e-05,
      "loss": 1.1157,
      "step": 1360
    },
    {
      "epoch": 1.9387610619469027,
      "grad_norm": 0.7905648350715637,
      "learning_rate": 1.7681775259678942e-05,
      "loss": 1.1707,
      "step": 1370
    },
    {
      "epoch": 1.952920353982301,
      "grad_norm": 0.7259710431098938,
      "learning_rate": 1.7445703493862133e-05,
      "loss": 1.1689,
      "step": 1380
    },
    {
      "epoch": 1.9670796460176991,
      "grad_norm": 0.6974449157714844,
      "learning_rate": 1.7209631728045327e-05,
      "loss": 1.1395,
      "step": 1390
    },
    {
      "epoch": 1.9812389380530973,
      "grad_norm": 0.6764417886734009,
      "learning_rate": 1.6973559962228517e-05,
      "loss": 1.1407,
      "step": 1400
    },
    {
      "epoch": 1.9953982300884956,
      "grad_norm": 0.7448476552963257,
      "learning_rate": 1.673748819641171e-05,
      "loss": 1.1175,
      "step": 1410
    },
    {
      "epoch": 2.008495575221239,
      "grad_norm": 0.7517534494400024,
      "learning_rate": 1.6501416430594902e-05,
      "loss": 1.1148,
      "step": 1420
    },
    {
      "epoch": 2.0226548672566373,
      "grad_norm": 0.8129080533981323,
      "learning_rate": 1.6265344664778093e-05,
      "loss": 1.1521,
      "step": 1430
    },
    {
      "epoch": 2.0368141592920352,
      "grad_norm": 0.7546329498291016,
      "learning_rate": 1.6029272898961284e-05,
      "loss": 1.111,
      "step": 1440
    },
    {
      "epoch": 2.0509734513274336,
      "grad_norm": 0.7669566869735718,
      "learning_rate": 1.5793201133144478e-05,
      "loss": 1.1266,
      "step": 1450
    },
    {
      "epoch": 2.065132743362832,
      "grad_norm": 0.7320416569709778,
      "learning_rate": 1.555712936732767e-05,
      "loss": 1.1393,
      "step": 1460
    },
    {
      "epoch": 2.0792920353982303,
      "grad_norm": 0.723910391330719,
      "learning_rate": 1.532105760151086e-05,
      "loss": 1.1526,
      "step": 1470
    },
    {
      "epoch": 2.093451327433628,
      "grad_norm": 0.6881905794143677,
      "learning_rate": 1.5084985835694052e-05,
      "loss": 1.1577,
      "step": 1480
    },
    {
      "epoch": 2.1076106194690265,
      "grad_norm": 0.753074049949646,
      "learning_rate": 1.4848914069877243e-05,
      "loss": 1.1744,
      "step": 1490
    },
    {
      "epoch": 2.121769911504425,
      "grad_norm": 0.8321221470832825,
      "learning_rate": 1.4612842304060433e-05,
      "loss": 1.1329,
      "step": 1500
    },
    {
      "epoch": 2.135929203539823,
      "grad_norm": 0.7429136037826538,
      "learning_rate": 1.4376770538243628e-05,
      "loss": 1.1122,
      "step": 1510
    },
    {
      "epoch": 2.150088495575221,
      "grad_norm": 0.8461129665374756,
      "learning_rate": 1.4140698772426818e-05,
      "loss": 1.1082,
      "step": 1520
    },
    {
      "epoch": 2.1642477876106194,
      "grad_norm": 0.7659701704978943,
      "learning_rate": 1.3904627006610011e-05,
      "loss": 1.1624,
      "step": 1530
    },
    {
      "epoch": 2.1784070796460178,
      "grad_norm": 0.776248574256897,
      "learning_rate": 1.3668555240793202e-05,
      "loss": 1.0959,
      "step": 1540
    },
    {
      "epoch": 2.192566371681416,
      "grad_norm": 0.827215313911438,
      "learning_rate": 1.3432483474976392e-05,
      "loss": 1.103,
      "step": 1550
    },
    {
      "epoch": 2.206725663716814,
      "grad_norm": 0.8020559549331665,
      "learning_rate": 1.3196411709159585e-05,
      "loss": 1.0998,
      "step": 1560
    },
    {
      "epoch": 2.2208849557522123,
      "grad_norm": 0.7571237087249756,
      "learning_rate": 1.2960339943342777e-05,
      "loss": 1.1923,
      "step": 1570
    },
    {
      "epoch": 2.2350442477876107,
      "grad_norm": 0.7794379591941833,
      "learning_rate": 1.272426817752597e-05,
      "loss": 1.2111,
      "step": 1580
    },
    {
      "epoch": 2.249203539823009,
      "grad_norm": 0.7046015858650208,
      "learning_rate": 1.248819641170916e-05,
      "loss": 1.1459,
      "step": 1590
    },
    {
      "epoch": 2.263362831858407,
      "grad_norm": 0.7245634198188782,
      "learning_rate": 1.2252124645892351e-05,
      "loss": 1.1367,
      "step": 1600
    },
    {
      "epoch": 2.2775221238938053,
      "grad_norm": 0.8425033688545227,
      "learning_rate": 1.2016052880075544e-05,
      "loss": 1.1048,
      "step": 1610
    },
    {
      "epoch": 2.2916814159292036,
      "grad_norm": 0.8309893608093262,
      "learning_rate": 1.1779981114258736e-05,
      "loss": 1.179,
      "step": 1620
    },
    {
      "epoch": 2.305840707964602,
      "grad_norm": 0.7649736404418945,
      "learning_rate": 1.1543909348441927e-05,
      "loss": 1.15,
      "step": 1630
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.840876042842865,
      "learning_rate": 1.1307837582625118e-05,
      "loss": 1.0798,
      "step": 1640
    },
    {
      "epoch": 2.334159292035398,
      "grad_norm": 0.8426408767700195,
      "learning_rate": 1.107176581680831e-05,
      "loss": 1.129,
      "step": 1650
    },
    {
      "epoch": 2.3483185840707965,
      "grad_norm": 0.8086553812026978,
      "learning_rate": 1.0835694050991501e-05,
      "loss": 1.1211,
      "step": 1660
    },
    {
      "epoch": 2.362477876106195,
      "grad_norm": 0.8163923621177673,
      "learning_rate": 1.0599622285174695e-05,
      "loss": 1.1487,
      "step": 1670
    },
    {
      "epoch": 2.3766371681415928,
      "grad_norm": 0.7197549343109131,
      "learning_rate": 1.0363550519357886e-05,
      "loss": 1.1131,
      "step": 1680
    },
    {
      "epoch": 2.390796460176991,
      "grad_norm": 0.8294039964675903,
      "learning_rate": 1.0127478753541077e-05,
      "loss": 1.1749,
      "step": 1690
    },
    {
      "epoch": 2.4049557522123894,
      "grad_norm": 0.8292040228843689,
      "learning_rate": 9.891406987724269e-06,
      "loss": 1.1187,
      "step": 1700
    },
    {
      "epoch": 2.419115044247788,
      "grad_norm": 0.8875254392623901,
      "learning_rate": 9.65533522190746e-06,
      "loss": 1.1502,
      "step": 1710
    },
    {
      "epoch": 2.4332743362831857,
      "grad_norm": 0.7455640435218811,
      "learning_rate": 9.419263456090652e-06,
      "loss": 1.1516,
      "step": 1720
    },
    {
      "epoch": 2.447433628318584,
      "grad_norm": 0.8237967491149902,
      "learning_rate": 9.183191690273845e-06,
      "loss": 1.1373,
      "step": 1730
    },
    {
      "epoch": 2.4615929203539824,
      "grad_norm": 0.8381979465484619,
      "learning_rate": 8.947119924457035e-06,
      "loss": 1.1182,
      "step": 1740
    },
    {
      "epoch": 2.4757522123893807,
      "grad_norm": 0.7974640130996704,
      "learning_rate": 8.711048158640226e-06,
      "loss": 1.1494,
      "step": 1750
    },
    {
      "epoch": 2.4899115044247786,
      "grad_norm": 0.7787233591079712,
      "learning_rate": 8.474976392823419e-06,
      "loss": 1.1333,
      "step": 1760
    },
    {
      "epoch": 2.504070796460177,
      "grad_norm": 0.8702592253684998,
      "learning_rate": 8.238904627006611e-06,
      "loss": 1.0923,
      "step": 1770
    },
    {
      "epoch": 2.5182300884955753,
      "grad_norm": 0.827316164970398,
      "learning_rate": 8.002832861189802e-06,
      "loss": 1.1406,
      "step": 1780
    },
    {
      "epoch": 2.532389380530973,
      "grad_norm": 0.7931164503097534,
      "learning_rate": 7.766761095372994e-06,
      "loss": 1.1156,
      "step": 1790
    },
    {
      "epoch": 2.5465486725663715,
      "grad_norm": 0.7793927788734436,
      "learning_rate": 7.530689329556186e-06,
      "loss": 1.1206,
      "step": 1800
    },
    {
      "epoch": 2.56070796460177,
      "grad_norm": 0.8721765875816345,
      "learning_rate": 7.294617563739377e-06,
      "loss": 1.1531,
      "step": 1810
    },
    {
      "epoch": 2.574867256637168,
      "grad_norm": 0.897468090057373,
      "learning_rate": 7.058545797922569e-06,
      "loss": 1.1189,
      "step": 1820
    },
    {
      "epoch": 2.5890265486725665,
      "grad_norm": 0.8683942556381226,
      "learning_rate": 6.822474032105761e-06,
      "loss": 1.1336,
      "step": 1830
    },
    {
      "epoch": 2.6031858407079644,
      "grad_norm": 0.8141804933547974,
      "learning_rate": 6.5864022662889514e-06,
      "loss": 1.1212,
      "step": 1840
    },
    {
      "epoch": 2.617345132743363,
      "grad_norm": 0.7958574891090393,
      "learning_rate": 6.350330500472145e-06,
      "loss": 1.1229,
      "step": 1850
    },
    {
      "epoch": 2.631504424778761,
      "grad_norm": 0.8187364935874939,
      "learning_rate": 6.1142587346553355e-06,
      "loss": 1.1401,
      "step": 1860
    },
    {
      "epoch": 2.645663716814159,
      "grad_norm": 0.8402313590049744,
      "learning_rate": 5.878186968838527e-06,
      "loss": 1.1072,
      "step": 1870
    },
    {
      "epoch": 2.6598230088495574,
      "grad_norm": 0.8630022406578064,
      "learning_rate": 5.642115203021719e-06,
      "loss": 1.1771,
      "step": 1880
    },
    {
      "epoch": 2.6739823008849557,
      "grad_norm": 0.7682392001152039,
      "learning_rate": 5.40604343720491e-06,
      "loss": 1.1385,
      "step": 1890
    },
    {
      "epoch": 2.688141592920354,
      "grad_norm": 0.8480066657066345,
      "learning_rate": 5.169971671388102e-06,
      "loss": 1.1287,
      "step": 1900
    },
    {
      "epoch": 2.7023008849557524,
      "grad_norm": 0.7792719006538391,
      "learning_rate": 4.9338999055712935e-06,
      "loss": 1.0929,
      "step": 1910
    },
    {
      "epoch": 2.7164601769911503,
      "grad_norm": 0.7613837122917175,
      "learning_rate": 4.697828139754486e-06,
      "loss": 1.1826,
      "step": 1920
    },
    {
      "epoch": 2.7306194690265486,
      "grad_norm": 0.8430830240249634,
      "learning_rate": 4.461756373937677e-06,
      "loss": 1.1275,
      "step": 1930
    },
    {
      "epoch": 2.744778761061947,
      "grad_norm": 0.7900350689888,
      "learning_rate": 4.225684608120869e-06,
      "loss": 1.2051,
      "step": 1940
    },
    {
      "epoch": 2.758938053097345,
      "grad_norm": 0.8770749568939209,
      "learning_rate": 3.989612842304061e-06,
      "loss": 1.0877,
      "step": 1950
    },
    {
      "epoch": 2.773097345132743,
      "grad_norm": 0.9015328288078308,
      "learning_rate": 3.753541076487253e-06,
      "loss": 1.1168,
      "step": 1960
    },
    {
      "epoch": 2.7872566371681415,
      "grad_norm": 0.7655372619628906,
      "learning_rate": 3.517469310670444e-06,
      "loss": 1.1484,
      "step": 1970
    },
    {
      "epoch": 2.80141592920354,
      "grad_norm": 0.8642213344573975,
      "learning_rate": 3.2813975448536356e-06,
      "loss": 1.1575,
      "step": 1980
    },
    {
      "epoch": 2.815575221238938,
      "grad_norm": 0.8703386783599854,
      "learning_rate": 3.045325779036827e-06,
      "loss": 1.1643,
      "step": 1990
    },
    {
      "epoch": 2.829734513274336,
      "grad_norm": 0.9941550493240356,
      "learning_rate": 2.809254013220019e-06,
      "loss": 1.1424,
      "step": 2000
    },
    {
      "epoch": 2.8438938053097345,
      "grad_norm": 0.7424459457397461,
      "learning_rate": 2.573182247403211e-06,
      "loss": 1.1036,
      "step": 2010
    },
    {
      "epoch": 2.858053097345133,
      "grad_norm": 0.8136595487594604,
      "learning_rate": 2.3371104815864024e-06,
      "loss": 1.1183,
      "step": 2020
    },
    {
      "epoch": 2.872212389380531,
      "grad_norm": 0.7852017879486084,
      "learning_rate": 2.101038715769594e-06,
      "loss": 1.0953,
      "step": 2030
    },
    {
      "epoch": 2.8863716814159295,
      "grad_norm": 0.8544552326202393,
      "learning_rate": 1.8649669499527859e-06,
      "loss": 1.14,
      "step": 2040
    },
    {
      "epoch": 2.9005309734513274,
      "grad_norm": 0.8779605031013489,
      "learning_rate": 1.6288951841359775e-06,
      "loss": 1.143,
      "step": 2050
    },
    {
      "epoch": 2.9146902654867257,
      "grad_norm": 0.8452448844909668,
      "learning_rate": 1.392823418319169e-06,
      "loss": 1.177,
      "step": 2060
    },
    {
      "epoch": 2.928849557522124,
      "grad_norm": 0.7984238266944885,
      "learning_rate": 1.1567516525023609e-06,
      "loss": 1.1537,
      "step": 2070
    },
    {
      "epoch": 2.943008849557522,
      "grad_norm": 0.856381356716156,
      "learning_rate": 9.206798866855524e-07,
      "loss": 1.1378,
      "step": 2080
    },
    {
      "epoch": 2.9571681415929203,
      "grad_norm": 0.8508303761482239,
      "learning_rate": 6.846081208687441e-07,
      "loss": 1.1736,
      "step": 2090
    },
    {
      "epoch": 2.9713274336283186,
      "grad_norm": 0.8312506079673767,
      "learning_rate": 4.485363550519358e-07,
      "loss": 1.1427,
      "step": 2100
    },
    {
      "epoch": 2.985486725663717,
      "grad_norm": 0.810690701007843,
      "learning_rate": 2.1246458923512751e-07,
      "loss": 1.1922,
      "step": 2110
    }
  ],
  "logging_steps": 10,
  "max_steps": 2118,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 6.979298408414577e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
